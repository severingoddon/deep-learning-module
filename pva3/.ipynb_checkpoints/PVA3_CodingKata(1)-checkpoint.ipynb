{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X42iRdsF4xBX"
   },
   "source": [
    "In dieser Coding Kata geht es um die Bildklassifikation mit tiefen neuronalen Netzen bzw. Convolutional Neural Nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "Successfully installed keras-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSFJ-ydg4xBI"
   },
   "source": [
    "## PVA 3 - Coding Kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdImCR_Q4xBY"
   },
   "source": [
    "__Aufgabe 0__\n",
    "\n",
    "Zum Aufwärmen eine kleine Mathematikaufgabe:\n",
    "\n",
    "Zeige, dass für die Aktivierungsfunktion $$f(x)=\\frac{e^{x}}{1+e^{x}}$$ folgende Eigenschaft gilt:\n",
    "$$f'(x)=f(x)(1-f(x))$$\n",
    "\n",
    "Hinweis: Ableitung unter Verwendung der Quotientenregel $(\\frac{h}{g})'=\\frac{h'g-g'h}{g^2}$\n",
    "\n",
    "Die **Lösungsschritte in LaTeX-Formatierung** hier eintragen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$f'(x)=\\frac{e^{x}*(1+e^{x}) - e^{x}*(e^{x})}{(1+e^{x})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x)=\\frac{e^{2x} + e^{x} - e^{2x}}{(1+e^{x})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x)=\\frac{e^{x}}{(1+e^{x})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folgende Eigenschaft gilt:\n",
    "$$f'(x)=f(x)(1-f(x))$$\n",
    "$$f'(x)=\\frac{e^{x}}{1+e^{x}}(1-\\frac{e^{x}}{1+e^{x}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x)=\\frac{e^{x}}{1+e^{x}}-\\frac{e^{2x}}{(1+e^{x})^{2}}$$\n",
    "erweitern links um $$1+e^{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x)=\\frac{e^{x}+e^{2x}-e^{2x}}{(1+e^{x})^{2}}$$\n",
    "ist gleich wie:\n",
    "$$f'(x)=\\frac{e^{x}}{(1+e^{x})^{2}}$$\n",
    "was aus dem obigen Schritt entstand. Daher ist es bewiesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tGWl-x14xBZ"
   },
   "source": [
    "__Aufgabe 1__\n",
    "- Arbeite folgende Google Colab Notebooks durch, die den FASHION_MNIST Datensatz klassifizieren\n",
    "    1. Keras (mit TensorFlow Backend) [Notebook](https://colab.research.google.com/github/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb)\n",
    "    2. Torch [Notebook](https://colab.research.google.com/drive/1XyLIdWFnWRSwXVOBorpkte70E06F_ITt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8Lyx03E4xBd"
   },
   "source": [
    "__Aufgabe 2__\n",
    "- Erstelle einen Bildklassifikator der folgende 6 Klassen erkennen kann:\n",
    "    1. Erdbeere\n",
    "    2. Berge\n",
    "    3. Dampflokomotive\n",
    "    4. Schäferhund\n",
    "    5. Dachs\n",
    "    6. Stapler\n",
    "    \n",
    "Hinweis: Die Klassen sind der ImageNet-Datenbank entnommen.\n",
    "\n",
    "Testbilder befinden sich auf unserem Google-Drive [PVA3](https://drive.google.com/drive/folders/1wyuTcLWeeXfdh0eAAYevcN5UFJeiX_XA?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AOcQNn74xBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt_SQGrJ4xBa"
   },
   "source": [
    "__Aufgabe 3__\n",
    "- Erstelle ein **eigenes** Convolutional Neural Network zur Klassifikation von Portraitbildern (Lächeln - Nicht-Lächeln)\n",
    "- Verwende dazu den [Smile-Datensatz](https://github.com/hromi/SMILEsmileD/tree/master/SMILEs), lade den Datensatz auf das eigene Google Drive (Mounting erfolgt mit nachfolgendem Code)\n",
    "- Verwende dazu die Keras Bibliothek mit TensorFlow-Backend (Google Colab mit GPU-Runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nPgyUwCE4xBb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10532 images belonging to 2 classes.\n",
      "Found 2633 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.8045WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F402732790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F402732790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "330/330 [==============================] - 215s 652ms/step - loss: 0.4365 - accuracy: 0.8045 - val_loss: 0.3366 - val_accuracy: 0.8538\n",
      "Epoch 2/10\n",
      "330/330 [==============================] - 213s 645ms/step - loss: 0.3240 - accuracy: 0.8646 - val_loss: 0.2984 - val_accuracy: 0.8773\n",
      "Epoch 3/10\n",
      "330/330 [==============================] - 212s 644ms/step - loss: 0.2903 - accuracy: 0.8834 - val_loss: 0.2772 - val_accuracy: 0.8864\n",
      "Epoch 4/10\n",
      "330/330 [==============================] - 213s 647ms/step - loss: 0.2790 - accuracy: 0.8862 - val_loss: 0.2620 - val_accuracy: 0.8914\n",
      "Epoch 5/10\n",
      "330/330 [==============================] - 213s 646ms/step - loss: 0.2688 - accuracy: 0.8912 - val_loss: 0.2931 - val_accuracy: 0.8845\n",
      "Epoch 6/10\n",
      "330/330 [==============================] - 217s 658ms/step - loss: 0.2575 - accuracy: 0.8960 - val_loss: 0.2787 - val_accuracy: 0.8891\n",
      "Epoch 7/10\n",
      "330/330 [==============================] - 215s 651ms/step - loss: 0.2440 - accuracy: 0.9028 - val_loss: 0.2540 - val_accuracy: 0.8910\n",
      "Epoch 8/10\n",
      "330/330 [==============================] - 215s 651ms/step - loss: 0.2221 - accuracy: 0.9116 - val_loss: 0.2558 - val_accuracy: 0.9035\n",
      "Epoch 9/10\n",
      "330/330 [==============================] - 215s 652ms/step - loss: 0.2032 - accuracy: 0.9190 - val_loss: 0.2887 - val_accuracy: 0.8918\n",
      "Epoch 10/10\n",
      "330/330 [==============================] - 216s 654ms/step - loss: 0.1738 - accuracy: 0.9315 - val_loss: 0.2997 - val_accuracy: 0.8792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f414e835e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define the image size and batch size\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create the data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Create the training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'SMILEsmileD-master/SMILEs/',\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='grayscale',  # set color_mode to 'grayscale'\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "# Create the validation data generator\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    'SMILEsmileD-master/SMILEs/',\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='grayscale',  # set color_mode to 'grayscale'\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),  # input_shape is now (IMG_SIZE[0], IMG_SIZE[1], 1)\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001F415C87E50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function trace_model_call.<locals>._wrapped_model at 0x000001F415C87E50>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001F415C87E50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function trace_model_call.<locals>._wrapped_model at 0x000001F415C87E50>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: aufg_3_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"aufg_3_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiahu7J24xBc"
   },
   "source": [
    "__Aufgabe 4__\n",
    "\n",
    "Klassifiziere den Smile-Datensatz unter Verwendung von _Transfer Learning_.\n",
    "Speichere das trainierte Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb1HI5md4xBe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Erstellen des Modells\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(img_size[0], img_size[1], 1),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define the image size and batch size\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create the data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Create the training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'SMILEsmileD-master/SMILEs/',\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='grayscale',  # set color_mode to 'grayscale'\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "# Create the validation data generator\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    'SMILEsmileD-master/SMILEs/',\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='grayscale',  # set color_mode to 'grayscale'\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kETX8B3N4xBe"
   },
   "source": [
    "__Aufgabe 5__\n",
    "\n",
    "Erstellen sie einen Bildklassifikator für den [Blumen-Datensatz](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/). Trainieren Sie ein vortrainiertes Netz ihrer Wahl auf diesen spezifischen Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vRo10Uul4xBe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1968\\2575405170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;34m\"Asked to retrieve element {idx}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;34m\"but the Sequence \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;34m\"has length {length}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             )\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "\n",
    "# Erstellen des Modells\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define the image size and batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create the data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Create the training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'jpg/',\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='rgb',  # set color_mode to 'grayscale'\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "# Create the validation data generator\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    'jpg/',\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='rgb',  # set color_mode to 'grayscale'\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(18, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=1, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
